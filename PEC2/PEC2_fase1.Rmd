---
title: "Endo-Mining: herramienta web para la búsqueda automatizada de genes potencialmente relacionados con la endometriosis a través de minería de textos en PubMed"
author: "Jorge Vallejo Ortega"
date: '`r format(Sys.Date(),"%d/%m/%Y")`'
output:
  pdf_document:
    number_sections: true
    toc: true
    # extra_dependencies: ["float"]
urlcolor: blue
linkcolor: blue
header-includes:
 - \renewcommand{\contentsname}{Índice}
    # - \usepackage{float}

# Next code for knitting into another directory with chosen filename comes from: https://stackoverflow.com/questions/39662365/knit-one-markdown-file-to-two-output-files/53280491#53280491
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
                    output_format = NULL,
                    output_dir = "results",
                    output_file = "vallejo_jorge_PEC2_fase1.pdf") })
# And:
# https://stackoverflow.com/a/46007686/10647267

bibliography: ../bibliography.bib
link-citations: yes
---
  
```{r setup, include=FALSE}
# knitr options

# Do not display code in output document
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = "center")
```

```{r estructura de directorios, results='hide', include=FALSE}
# 'data' contains raw source data.
# 'intermediateData' contains .RData objects with processed data.
# 'results' stores the final report files.

directories <- c("data", "results", "intermediateData", "images")

# Create directories
lapply(directories, function(x){
  if (!(dir.exists(x))){
    dir.create(x)
  }
})
```

```{r delete results files, eval= FALSE, include=FALSE}
# Run this chunk ONLY if you want to re-do
# the complete the report FROM THE ORIGINAL DATA.
# Remember that the .RData files are there to
# avoid unnecesarily redoing of long data processing.

directories <- c("results/", "intermediateData/", "images/")

file.remove(
# Create a character vector of relative paths
# to all files in the variable directories
  list.files(path = directories,
             all.files = TRUE,
             full.names = TRUE,
             recursive = TRUE)
)
```

```{r libraries, include=FALSE}
# Load packages
library(knitr)
library(pubmed.mineR)
library(easyPubMed)
```

# Búsqueda en PubMed

PubMed es un recurso en línea de acceso público y gratuito consistente en una base de datos - en continuo crecimiento - que incluye más de 32 millones de citas y abstracts de literatura biomédica, tanto artículos (_MEDLINE_ y _PubMed Central_) como libros (_Bookshelf_). Esta base de datos - en línea desde 1996 - fue desarrollada y sigue siendo mantenida por el Centro Nacional para la Información Biotecnológica (_National Center for Biotechnology Information_, NCBI), que forma parte de la Biblioteca Nacional de Medicina de los E.E.U.U. (_U.S. National Library of Medicine_, NLM) de los Institutos Nacionales de Salud (_National Institutes of Health_, NIH). Esta base de datos está especializada en publicaciones centradas en campos científicos relacionados con la salud y la biomedicina (@national_library_of_medicine_about_nodate, @national_library_of_medicine_medline_nodate).

```{r pubmed_homepage, fig.cap="Página principal de PubMed. La barra de búsqueda destaca en medio de la imagen, indicando la finalidad principal de esta página.", out.width=200}
include_graphics(path = "images/pubmed_homepage.png")
```

Para realizar búsquedas es posible utilizar una serie de etiquetas con las que especificar si el término que hemos escrito debe buscarse como parte del título (etiqueta [TI]), el autor ([AU]), la revista ([TA]) o cualquier otro campo dentro de una larga lista que podemos consultar en la sección de ayuda de PubMed[^1]. También es posible usar los operadores booleanos AND, OR y NOT. Sin embargo, no es necesario emplear las etiquetas ni los operadores, ya que el motor de búsqueda de la página puede crear por sí mismo búsquedas complejas a partir de sólo las palabras clave que introducimos en el campo de búsqueda.

[^1]: <https://pubmed.ncbi.nlm.nih.gov/help/#search-tags>

El algoritmo que construye las búsquedas a partir de nuestras palabras clave (_Automatic Term Mapping_) contrasta dichas palabras clave contra diferentes tablas de traducción de términos. En este orden: tabla de traducción de temas, tabla de traducción de revistas, índice de autores e índice de investigadores (colaboradores). Cuando se encuentra una coincidencia para el término o la frase, dicha coincidencia se añade a la búsqueda y no se continúa en la siguiente tabla de traducción.

La tabla de temas relaciona - entre otras cosas - las diferentes formas ortográficas del inglés americano y el británico, formas singulares y plurales, sinónimos, términos fuertemente relacionados, nombres de medicamentos genéricos y sus nombres comerciales, y el vocabulario controlado incluido en el tesauro MeSH (_Medical Subject Headings).

La tabla de revistas contiene y relaciona el título completo de las revistas, sus abreviaciones y sus números ISSN.

El índice de autores y el índice de investigadores contienen el nombre, iniciales y nombre completo de los autores incluidos en la base de datos.


Primero de todo pruebo a realizar una búsqueda en PubMed (https://pubmed.ncbi.nlm.nih.gov/). La más básica posible y más general, sin ningún filtro, con la palabra clave "endometriosis". El resultado son 29,361 citas, desde 1927 hasta 2021.

```{r pubmed_search, fig.cap="Página de resultados para el término 'endometriosis'. En la parte central se muestran las citas recuperadas por el algoritmo. En el margen izquierdo se nos ofrecen filtros interactivos para refinar la búsqueda.", out.width=200}
include_graphics(path = "images/pubmed_search.png")
```

A través del enlace 'Advanced', que se encuentra en la zona superior izquierda, podemos acceder a un constructor de búsquedas que nos facilita hacer búsquedas avanzadas sin necesidad de conocer todas las etiquetas disponibles. En esa misma página podemos consultar nuestro historial de búsquedas recientes y, en éste, cómo el constructor de búsquedas ha traducido nuestra búsqueda simple ('endometriosis') utilizando las tablas de traducción:

```{r pubmed_search_translation, fig.cap="Detalle del historial de búsqueda. De izquierda a derecha muestra la siguiente información: número de la búsqueda en orden cronológico, los términos de búsqueda entrados por el usuario y los términos a los que el algoritmo de mapeo automático los ha traducido, el número de resultados y finalmente la hora a la que se solicitó la búsqueda.", out.width=200}
include_graphics(path = "images/pubmed_homepage.png")
```

Asimismo si usamos los filtros para, por ejemplo, limitar la búsqueda a los artículos publicados durante los últimos diez años, también podemos consultar la estructura de dicha búsqueda:

```{r pubmed_search_filtro, fig.cap="Detalle del historial de búsqueda. Misma búsqueda del ejemplo anterior, filtrada para obtener como resultado citas de entre los años 2010 y 2021", out.width=200}
include_graphics(path = "images/pubmed_search_filtro.png")
```

# Exploración de paquetes de minería de textos

## _easyPubMed_

El paquete _easyPubMed_ es una interfaz que permite usar R para interactuar con las **Entrez Programming Utilities**, las API[^2] públicas que permiten el acceso programático a las bases de datos Entrez (PubMed, PMC, Gene, Nuccore y Protein). Las funciones de este paquete permiten la descarga por lotes de grandes volúmenes de registros, y el procesado básico del resultado de las búsquedas en PubMed.

[^2]: Siglas en inglés de interfaz de programación de aplicaciones (_application programming interface_), que se refiere al conjunto de funciones y protocolos que un programa ofrece para poder ser usado por otro programa diferente.

La función principal que usaremos de este paquete es `batch_pubmed_download()`, que permite realizar una búsqueda en PubMed y descargar los resultados en forma de ficheros. Los resultados se pueden descargar en formato XML o TXT en lotes de hasta 5 000 registros. Estos resultados en formato texto conforman los datos sobre los que usaremos las funciones del paquete _pubmed.mineR_.

Como ejemplo, descargaremos los registros correspondientes al término de búsqueda "endometriosis" con fecha de publicación posterior a 2019:

```{r batch download, eval=FALSE}
query <- "endometriosis AND 2020/01/01:3000/12/31[dp]"

batch_pubmed_download(pubmed_query_string = query,
                      dest_dir = "intermediateData/",
                      dest_file_prefix = "last_endometriosis",
                      format = "abstract",
                      batch_size = 5000
                      )
```

El comporatmiento de la función `batch_pubmed_download()` se puede ajustar mediante diferentes opciones, algunas de las cuales se pueden ver en este ejemplo. Mediante *pubmed_query_string* especificamos los términos con los que se efectuará la búsqueda en PubMed. Puede ser una búsqueda sencilla sólo con los término o, como en el ejemplo, contener etiquetas (ej. [dp], fecha de publicación) y operadores booleanos (ej. AND). El directorio de destino se puede elegir mediante la opción *dest_dir*, y la opción *dest_file_prefix* nos permite elegir el prefijo que se añadirá a cada uno de los ficheros creados para almacenar los resultados. Con la opción *batch_size* podemos elegir el número máximo de registros incluidos en cada fichero (entre 1 y 5 000). Por último, la opción *format* nos da la oportunidad de elegir el formato en el que recibiremos los datos. El formato XML es rico en información y permite un procesado posterior más complejo del corpus primario (p.ej. subdividiéndolo por fecha, por autor, u otras opciones). Nosotros sin embargo hemos elegido una de las opciones en formato texto, ya que no necesitaremos realizar ese tipo de procesado del corpus y además resultará en ficheros que, conteniendo el mismo número de registro, ocuparán menos espacio de memoria.


```{r pubmed_search_filtro, out.width=200}
include_graphics(path = "images/batch_pubmed_download.png")
```



## _pubmed.mineR_

El paquete _pubmed.mineR_, para el lenguage **R**, se ha desarrollado específicamente para facilitar la minería de textos en el ámbito de la investigación biomédica; concretamente, aplicada a los sumarios (*abstracts*) de artículos incluidos en las citas de la base de datos PubMed. Para este fin, incluye multitud de herramientas que implementan algoritmos de minería de textos o que usan herramientas ya existentes en otros paquetes. En esta sección comentaremos, de entre las muchas funciones contenidas en el paquete, tan sólo aquellas que nos resultarán de utilidad en este trabajo.

En primer lugar, para constituir el **corpus primario**, utilizaremos la función `readabs()` sobre el archivo en formato texto que contiene los registros resultado de nuestra búsqueda previa. El resultado es un objeto tipo S4 con tres _slots_ que contienen, respectivamente, la información referente al título de la revista, el texto del sumario y el código PMID del artículo.

```{r ejemplo readbs}
last_abstracts <- readabs("intermediateData/last_endometriosis01.txt")

str(last_abstracts, vec.len = 1)
```

Disponemos de dos importantes funciones para el **reconomiento de entidades**. La función `gene_atomization()` reconoce los nombres de los genes (en su codificación como símbolo HGNC) y los extrae del del corpus primario además de calcular sus frecuencias.

```{r}
last_genes <- gene_atomization(last_abstracts)
head(last_genes)
```

Por otro lado, la función `word_atomizations()` es más general. Disgrega el texto en palabras y las ordena según su frecuencia. No tiene en cuenta los espacios, la puntuación ni las palabras más comunes del idioma inglés.
```{r word atomizations example, eval=FALSE}
last_words <- word_atomizations(last_abstracts)
head(last_words)
```

Finalmente, la función `tdm_for_lsa()`, a partir de un vector de términos, encuentra la frecuencia de cada término en cada uno de los sumarios del corpus primario. Devuelve una **matriz documento-término** con las frecuencias de los términos buscados en la que cada fila representa uno de los términos y cada columna representa un documento (en este caso, un sumario). Esta matriz se puede usar posteriormente para realizar un análisis semántico latente.

```{r term document matrix example, eval=FALSE}
tdm <- tdm_for_lsa(last_abstracts, c("age", "gender", "woman", "women", " man ", " men ", "smoking", "relationships", "relation"))
tdm[, 1:10]
```




# Referencias
\