---
title: "Endo-Mining: herramienta web para la búsqueda automatizada de genes potencialmente relacionados con la endometriosis a través de minería de textos en PubMed"
author: "Jorge Vallejo Ortega"
date: '`r format(Sys.Date(),"%d/%m/%Y")`'
output:
  pdf_document:
    number_sections: true
    toc: true
    toc_depth: 2
    # extra_dependencies: ["float"]
urlcolor: blue
linkcolor: blue
indent: true
header-includes:
 - \renewcommand{\contentsname}{Índice}
 - \usepackage{float}
 - \floatplacement{figure}{H}
 - \usepackage{caption}
 - \captionsetup[figure]{name=Figura}

# Next code for knitting into another directory with chosen filename comes from: https://stackoverflow.com/questions/39662365/knit-one-markdown-file-to-two-output-files/53280491#53280491
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding,
                    output_format = NULL,
                    output_dir = "results",
                    output_file = "vallejo_jorge_PEC2_fase1.pdf") })
# And:
# https://stackoverflow.com/a/46007686/10647267

bibliography: ../bibliography.bib
link-citations: yes
---
  
```{r setup, include=FALSE}
# knitr options

# Do not display code in output document
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      fig.align = "center")
```

```{r estructura de directorios, results='hide', include=FALSE}
# 'data' contains raw source data.
# 'intermediateData' contains .RData objects with processed data.
# 'results' stores the final report files.

directories <- c("data", "results", "intermediateData", "images")

# Create directories
lapply(directories, function(x){
  if (!(dir.exists(x))){
    dir.create(x)
  }
})
```

```{r delete results files, eval= FALSE, include=FALSE}
# Run this chunk ONLY if you want to re-do
# the complete the report FROM THE ORIGINAL DATA.
# Remember that the .RData files are there to
# avoid unnecesarily redoing of long data processing.

directories <- c("results/", "intermediateData/", "images/")

file.remove(
# Create a character vector of relative paths
# to all files in the variable directories
  list.files(path = directories,
             all.files = TRUE,
             full.names = TRUE,
             recursive = TRUE)
)
```

```{r libraries, include=FALSE}
# Load packages
library(knitr)
library(pubmed.mineR)
library(easyPubMed)
```
\newpage

# Búsqueda en PubMed  

PubMed es un recurso en línea de acceso público y gratuito consistente en una base de datos - en continuo crecimiento - que incluye más de 32 millones de citas y abstracts de literatura biomédica, tanto artículos (_MEDLINE_ y _PubMed Central_) como libros (_Bookshelf_). Esta base de datos - en línea desde 1996 - fue desarrollada y sigue siendo mantenida por el Centro Nacional para la Información Biotecnológica (_National Center for Biotechnology Information_, NCBI), que forma parte de la Biblioteca Nacional de Medicina de los E.E.U.U. (_U.S. National Library of Medicine_, NLM) de los Institutos Nacionales de Salud (_National Institutes of Health_, NIH). Esta base de datos está especializada en publicaciones centradas en campos científicos relacionados con la salud y la biomedicina (@national_library_of_medicine_about_nodate, @national_library_of_medicine_medline_nodate).

```{r pubmed_homepage, fig.cap="Página principal de PubMed. La barra de búsqueda destaca en medio de la imagen, indicando la finalidad principal de esta página.", out.width='50%'}
include_graphics(path = "images/pubmed_homepage.png")
```

Para realizar búsquedas es posible utilizar una serie de etiquetas con las que especificar si el término que hemos escrito debe buscarse como parte del título (etiqueta [TI]), el autor ([AU]), la revista ([TA]) o cualquier otro campo dentro de una larga lista que podemos consultar en la sección de ayuda de PubMed[^1]. También es posible usar los operadores booleanos AND, OR y NOT. Sin embargo, no es necesario emplear las etiquetas ni los operadores, ya que el motor de búsqueda de la página puede crear por sí mismo búsquedas complejas a partir de sólo las palabras clave que introducimos en el campo de búsqueda.

[^1]: <https://pubmed.ncbi.nlm.nih.gov/help/#search-tags>

El algoritmo que construye las búsquedas a partir de nuestras palabras clave (_Automatic Term Mapping_) contrasta dichas palabras clave contra diferentes tablas de traducción de términos. En este orden: tabla de traducción de temas, tabla de traducción de revistas, índice de autores e índice de investigadores (colaboradores). Cuando se encuentra una coincidencia para el término o la frase, dicha coincidencia se añade a la búsqueda y no se continúa en la siguiente tabla de traducción.

La tabla de temas relaciona - entre otras cosas - las diferentes formas ortográficas del inglés americano y el británico, formas singulares y plurales, sinónimos, términos fuertemente relacionados, nombres de medicamentos genéricos y sus nombres comerciales, y el vocabulario controlado incluido en el tesauro MeSH (*Medical Subject Headings*).

La tabla de revistas contiene y relaciona el título completo de las revistas, sus abreviaciones y sus números ISSN.

El índice de autores y el índice de investigadores contienen el nombre, iniciales y nombre completo de los autores incluidos en la base de datos.


Primero de todo pruebo a realizar una búsqueda en PubMed (https://pubmed.ncbi.nlm.nih.gov/). La más básica posible y más general, sin ningún filtro, con la palabra clave "endometriosis". El resultado son 29,361 citas, desde 1927 hasta 2021.

```{r pubmed_search, fig.cap="Página de resultados para el término 'endometriosis'. En la parte central se muestran las citas recuperadas por el algoritmo. En el margen izquierdo se nos ofrecen filtros interactivos para refinar la búsqueda."}
include_graphics(path = "images/pubmed_search.png")
```

A través del enlace 'Advanced', que se encuentra en la zona superior izquierda, podemos acceder a un constructor de búsquedas que nos facilita hacer búsquedas avanzadas sin necesidad de conocer todas las etiquetas disponibles. En esa misma página podemos consultar nuestro historial de búsquedas recientes y, en éste, cómo el constructor de búsquedas ha traducido nuestra búsqueda simple ('endometriosis') utilizando las tablas de traducción:

```{r pubmed_search_translation, fig.cap="Detalle del historial de búsqueda. De izquierda a derecha muestra la siguiente información: número de la búsqueda en orden cronológico, los términos de búsqueda entrados por el usuario y los términos a los que el algoritmo de mapeo automático los ha traducido, el número de resultados y finalmente la hora a la que se solicitó la búsqueda."}
include_graphics(path = "images/pubmed_search_translation.png")
```

Asimismo si usamos los filtros para, por ejemplo, limitar la búsqueda a los artículos publicados durante los últimos diez años, también podemos consultar la estructura de dicha búsqueda:

```{r pubmed_search_filtro, fig.cap="Detalle del historial de búsqueda. Misma búsqueda del ejemplo anterior, filtrada para obtener como resultado citas de entre los años 2010 y 2021"}
include_graphics(path = "images/pubmed_search_filtro.png")
```

# Exploración de paquetes de minería de textos  

## _easyPubMed_  

El paquete _easyPubMed_ es una interfaz que permite usar R para interactuar con las **Entrez Programming Utilities**, las API[^2] públicas que permiten el acceso programático a las bases de datos Entrez (PubMed, PMC, Gene, Nuccore y Protein). Las funciones de este paquete permiten la descarga por lotes de grandes volúmenes de registros, y el procesado básico del resultado de las búsquedas en PubMed.

[^2]: Siglas en inglés de interfaz de programación de aplicaciones (_application programming interface_), que se refiere al conjunto de funciones y protocolos que un programa ofrece para poder ser usado por otro programa diferente.

La función principal que usaremos de este paquete es `batch_pubmed_download()`, que permite realizar una búsqueda en PubMed y descargar los resultados en forma de ficheros. Los resultados se pueden descargar en formato XML o TXT en lotes de hasta 5 000 registros. Estos resultados en formato texto conforman los datos sobre los que usaremos las funciones del paquete _pubmed.mineR_.

Como ejemplo, descargaremos los registros correspondientes al término de búsqueda "endometriosis" con fecha de publicación posterior a 2019:

```{r batch download, eval=FALSE, echo=TRUE}
query <- "endometriosis AND 2020/01/01:3000/12/31[dp]"

batch_pubmed_download(pubmed_query_string = query,
                      dest_dir = "intermediateData/",
                      dest_file_prefix = "last_endometriosis",
                      format = "abstract",
                      batch_size = 5000
                      )
```

El comporatmiento de la función `batch_pubmed_download()` se puede ajustar mediante diferentes opciones, algunas de las cuales se pueden ver en este ejemplo. Mediante *pubmed_query_string* especificamos los términos con los que se efectuará la búsqueda en PubMed. Puede ser una búsqueda sencilla sólo con los término o, como en el ejemplo, contener etiquetas (ej. [dp], fecha de publicación) y operadores booleanos (ej. AND).  

El directorio de destino se puede elegir mediante la opción *dest_dir*, y la opción *dest_file_prefix* nos permite elegir el prefijo que se añadirá a cada uno de los ficheros creados para almacenar los resultados.  

Con la opción *batch_size* podemos elegir el número máximo de registros incluidos en cada fichero (entre 1 y 5.000). Por último, la opción *format* nos da la oportunidad de elegir el formato en el que recibiremos los datos. El formato XML es rico en información y permite un procesado posterior más complejo del corpus primario (p.ej. subdividiéndolo por fecha, por autor, u otras opciones). Nosotros sin embargo hemos elegido una de las opciones en formato texto, ya que no necesitaremos realizar ese tipo de procesado del corpus y además resultará en ficheros que, conteniendo el mismo número de registro, ocuparán menos espacio de memoria.





## _pubmed.mineR_  

El paquete _pubmed.mineR_, para el lenguage **R**, se ha desarrollado específicamente para facilitar la minería de textos en el ámbito de la investigación biomédica; concretamente, aplicada a los sumarios (*abstracts*) de artículos incluidos en las citas de la base de datos PubMed. Para este fin, incluye multitud de herramientas que implementan algoritmos de minería de textos o que usan herramientas ya existentes en otros paquetes. En esta sección comentaremos, de entre las muchas funciones contenidas en el paquete, tan sólo aquellas que nos resultarán de utilidad en este trabajo.

En primer lugar, para constituir el **corpus primario**, utilizaremos la función `readabs()` sobre el archivo en formato texto que contiene los registros resultado de nuestra búsqueda previa. El resultado es un objeto tipo S4 con tres _slots_ que contienen, respectivamente, la información referente al título de la revista, el texto del sumario y el código PMID[^3] del artículo.

[^3]: PubMed ID; número de identificación único asignado a cada una de las referencias incluidas en la base de datos PubMed.

```{r ejemplo readbs, eval=FALSE, echo=TRUE}
last_abstracts <- readabs("intermediateData/last_endometriosis01.txt")
```
```{r load last abstracts object}
load("intermediateData/last_abstracts.RData")
```
```{r last abstracts structure, echo=TRUE}
str(last_abstracts, vec.len = 1, nchar.max = 50)
```

Disponemos de dos importantes funciones para el **reconomiento de entidades**. La función `gene_atomization()` reconoce los nombres de los genes (en su codificación como símbolo HGNC) y los extrae del del corpus primario además de calcular sus frecuencias.

```{r ejemplo gene atomization, eval=FALSE, echo=TRUE}
last_genes <- gene_atomization(last_abstracts)
```
```{r load last genes}
load("intermediateData/last_genes.RData")
```
```{r ejemplo last genes, echo=TRUE}
head(last_genes)
```

Por otro lado, la función `word_atomizations()` es más general. Disgrega el texto en palabras y las ordena según su frecuencia. No tiene en cuenta los espacios, la puntuación ni las palabras más comunes del idioma inglés.
```{r word atomizations example, eval=FALSE, echo=TRUE}
last_words <- word_atomizations(last_abstracts)
```
```{r load word atomizations example}
load("intermediateData/last_words.RData")
```
```{r word atomizations print, echo=TRUE}
head(last_words)
```

Finalmente, la función `tdm_for_lsa()`, a partir de un vector de términos, encuentra la frecuencia de cada término en cada uno de los sumarios del corpus primario. Devuelve una **matriz documento-término** con las frecuencias de los términos buscados en la que cada fila representa uno de los términos y cada columna representa un documento (en este caso, un sumario). Esta matriz se puede usar posteriormente para realizar un análisis semántico latente.

```{r term document matrix example, eval=FALSE, echo=TRUE}
tdm <- tdm_for_lsa(last_abstracts, 
                   c("age", "gender", "woman", "women", " man ", 
                     " men ", "smoking", "relationships", "relation"))
```
```{r load example tdm}
load("intermediateData/last_tdm.RData")
```
```{r tdm example print, echo=TRUE}
tdm[, 1:10]
```

# Generación del corpus primario

El corpus primario para la actividad de minería de texto consistirá en todos los sumarios recuperados de la base de datos PubMed utilizando como término de búsqueda el término "endometriosis". Los registros resultado de la búsqueda se descargarán por lotes en forma de varios archivos de texto, cuyo contenido se reunirá en un único archivo. A partir de este archivo refundido se generará un objeto S4 de clase 'Abstract' conteniendo - de cada registro recuperado - el título, el texto del sumario y el código PMID. Este objeto será lo que consideraremos como corpus primario y la información que tomaremos de partida en los métodos de minería de textos.

## Solicitud de búsqueda y descarga de los registros

Como señalábamos anteriormente, usaremos la función `batch_pubmed_download()` para enviar nuestra búsqueda a la base de datos, recuperar registros y guardarlos en formato texto repartidos en varios ficheros:

```{r download all endometriosis, eval=FALSE, echo=TRUE}
batch_pubmed_download("endometriosis",
                      dest_dir = "data/",
                      dest_file_prefix = "total_endometriosis",
                      format = "abstract",
                      batch_size = 5000
                      )
```

Previamente, al hacer la búsqueda de prueba en la página de PubMed ya habíamos visto que se recuperan casi 30.000 registros. Lo que significa que el tiempo de descarga será relativamente largo y, al haber especificado que se generaría un archivo por cada 5.000 registros, tendremos como resultado 6 archivos de texto conteniendo todos los registros:


```{r pubmed batch download}
include_graphics(path = "images/batch_pubmed_download.png")
```

Crearemos un nuevo fichero de texto y copiaremos en él todos los registros, que están repartidos entre los ficheros anteriores:

```{r concatenate text files, eval=FALSE}
# List of files to be added together
files_list <- list.files(path = "data/",
                         pattern = "total",
                         full.names = TRUE) # include path
# Create new file
out_file <- file(description = "intermediateData/todos.txt",
                 open = "w")
# Read each downloaded file and write into final file
for (i in files_list){
  x <- readLines(i)
  writeLines(x, out_file)
}

close(out_file)
```

El resultado será un fichero llamado `todos.txt` conteniendo todos los registros resultado de la búsqueda, en formato TXT. A partir de la información contenida en este fichero generaremos el objeto de clase 'Abstracts' que pueden manipular las funciones del paquete _pubmed.mineR_:

```{r generate abstract object, eval=FALSE}
# Generate the object
abstracts <- readabs("intermediateData/todos.txt")
```

```{r load abstract object}
load("intermediateData/abstracts.RData")
```


Si examinamos la estructura del objeto:
```{r estructura de objeto abstracts}
str(abstracts, vec.len = 1, nchar.max = 50)
```

Vemos que consta de 3 _slots_, dos de ellos almacenando datos de tipo cadena de caracteres (título del artículo y su sumario, respectivamente), y un último _slot_ de tipo numérico almacenando el código PMID. Es a la información contenida en este objeto a la que aplicaremos los métodos de minería de textos.



# Referencias  
\